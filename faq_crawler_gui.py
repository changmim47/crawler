import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import asyncio
from datetime import datetime, timedelta
import tempfile
import os
import gspread
from google.oauth2.service_account import Credentials
from playwright.async_api import async_playwright

# -------------------- QID ì¤‘ë³µ í•„í„° --------------------
def load_collected_qids(file_path="collected_qids.txt"):
    try:
        with open(file_path, "r") as f:
            return set(line.strip() for line in f)
    except FileNotFoundError:
        return set()

def save_new_qids(new_qids, file_path="collected_qids.txt"):
    with open(file_path, "a") as f:
        for qid in new_qids:
            f.write(f"{qid}\n")

# -------------------- QID ìˆ˜ì§‘ --------------------
def collect_qna_ids(html):
    soup = BeautifulSoup(html, "html.parser")
    qna_elements = soup.select('td.subject[onclick^="fnProperties("]')
    ids = [elem.get("onclick").split("'")[1] for elem in qna_elements]
    return ids

# -------------------- ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ --------------------
def get_total_pages(html):
    soup = BeautifulSoup(html, "html.parser")
    page_links = soup.select("td.pagenum a[href*='page=']")
    pages = []
    for link in page_links:
        href = link.get("href")
        if "page=" in href:
            try:
                page_num = int(href.split("page=")[-1])
                pages.append(page_num)
            except ValueError:
                continue
    return max(pages) if pages else 1

# -------------------- ì§ˆë¬¸/ë‹µë³€ ì¶”ì¶œ --------------------
def extract_question(soup):
    h2_tags = soup.select("div.infor_customer > h2")
    for h2 in h2_tags:
        if "ì§ˆë¬¸ë‚´ìš©" in h2.text:
            content_div = h2.find_next_sibling("div")
            return content_div.get_text(strip=True) if content_div else ""
    return ""

def extract_answer(soup):
    h2_tags = soup.select("div.infor_customer > h2")
    for h2 in h2_tags:
        if "ë‹µë³€ë‚´ì—­" in h2.text:
            answer_paragraphs = []
            for p in h2.find_all_next("p"):
                if p.find_previous("h2") != h2:
                    break
                answer_paragraphs.append(p.get_text(strip=True))
            return "\n".join(answer_paragraphs)
    return ""

# -------------------- êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ --------------------
def upload_to_google_sheet(sheet_name, df):
    scope = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    creds = Credentials.from_service_account_info(st.secrets["gcp_service_account"], scopes=scope)
    gc = gspread.authorize(creds)

    try:
        sh = gc.open(sheet_name)
    except gspread.SpreadsheetNotFound:
        st.error("âŒ êµ¬ê¸€ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    worksheet = sh.sheet1
    worksheet.clear()
    worksheet.update([df.columns.values.tolist()] + df.values.tolist())

# -------------------- í¬ë¡¤ë§ --------------------
async def crawl_faq(username, password, base_url, existing_qids):
    results = []
    new_qids = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        # ë¡œê·¸ì¸
        await page.goto("https://faq.megagong.net/")
        await page.fill("input[name='USERID']", username)
        await page.fill("input[name='USER_PWD']", password)
        await page.evaluate("go_submit();")
        await page.wait_for_selector("#sStartDate")

        await page.goto(base_url)
        await page.wait_for_selector("td.subject[onclick^='fnProperties']")
        content = await page.content()
        total_pages = get_total_pages(content)

        for page_num in range(1, total_pages + 1):
            url = base_url + f"&page={page_num}"
            await page.goto(url)
            await page.wait_for_selector("td.subject[onclick^='fnProperties']")
            html = await page.content()
            qna_ids = collect_qna_ids(html)
            soup = BeautifulSoup(html, "html.parser", from_encoding="euc-kr")

            for qid in qna_ids:
                if qid in existing_qids:
                    continue

                await page.goto(f"https://faq.megagong.net/erms/transmissionM/properties.asp?intQstIdx={qid}&page=1")
                await page.wait_for_selector("div.infor_customer")
                q_html = await page.content()
                q_soup = BeautifulSoup(q_html, "html.parser", from_encoding="euc-kr")
                q = extract_question(q_soup)

                await page.goto(f"https://faq.megagong.net/erms/transmissionM/properties_02.asp?intQstIdx={qid}&page=1")
                await page.wait_for_selector("div.infor_customer")
                a_html = await page.content()
                a_soup = BeautifulSoup(a_html, "html.parser", from_encoding="euc-kr")
                a = extract_answer(a_soup)

                results.append({"QID": qid, "ì§ˆë¬¸": q, "ë‹µë³€": a})
                new_qids.append(qid)

        await browser.close()
    return results, new_qids

# -------------------- Streamlit ì¸í„°í˜ì´ìŠ¤ --------------------
st.title("ğŸ“Œ ë©”ê°€ê³µ FAQ ìë™ ìˆ˜ì§‘ê¸° (Playwright)")

username = st.text_input("ğŸ” ê´€ë¦¬ì ID")
password = st.text_input("ğŸ” ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password")
criterion = st.radio("ğŸ“… ê¸°ì¤€ ì„ íƒ", ["ì§ˆë¬¸ì¼ ê¸°ì¤€", "ë‹µë³€ì¼ ê¸°ì¤€"])
rCriterion = "1" if criterion == "ì§ˆë¬¸ì¼ ê¸°ì¤€" else "2"
start_date = st.date_input("ì‹œì‘ì¼", value=datetime.today() - timedelta(days=1))
start_hour = st.selectbox("ì‹œì‘ ì‹œê°„ (0~23ì‹œ)", list(range(24)), index=0)
end_date = st.date_input("ì¢…ë£Œì¼", value=datetime.today())
end_hour = st.selectbox("ì¢…ë£Œ ì‹œê°„ (0~23ì‹œ)", list(range(24)), index=23)
filter_duplicates = st.checkbox("âœ… ê¸°ì¡´ì— ìˆ˜ì§‘í•œ QIDëŠ” ì œì™¸í•˜ê³  ìˆ˜ì§‘", value=True)
sheet_name = st.text_input("ğŸ“„ ì—…ë¡œë“œí•  Google ì‹œíŠ¸ ì´ë¦„")
auto_upload = st.checkbox("ğŸ“¡ í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ ì—…ë¡œë“œ", value=False)

if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘"):
    with st.spinner("í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        base_url = (
            "https://faq.megagong.net/erms/transmissionM/index.asp"
            f"?rCriterion={rCriterion}"
            f"&sStartDate={start_date.strftime('%Y-%m-%d')}"
            f"&sStartHour={start_hour:02}"
            f"&sEndDate={end_date.strftime('%Y-%m-%d')}"
            f"&sEndHour={end_hour:02}"
        )

        existing_qids = load_collected_qids() if filter_duplicates else set()

        results, new_qids = asyncio.run(crawl_faq(username, password, base_url, existing_qids))

        if new_qids:
            save_new_qids(new_qids)

        df = pd.DataFrame(results)
        st.success(f"ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
            df.to_excel(tmp.name, index=False)
            tmp.seek(0)
            st.download_button("ğŸ“… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", tmp.read(), file_name="qna_data.xlsx")

        if auto_upload and sheet_name:
            try:
                upload_to_google_sheet(sheet_name, df)
                st.success("âœ… êµ¬ê¸€ ì‹œíŠ¸ ìë™ ì—…ë¡œë“œ ì™„ë£Œ!")
            except Exception as e:
                st.error(f"âŒ ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
